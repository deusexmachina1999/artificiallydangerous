<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="logos.css">
    <title>LOGOS</title>
</head>

<body>
    <header><b>Artificially Dangerous: AI and its detrimental effects on the world</b></header>
    <nav>
        <a href="index.html" id="first"> A brief history of AI</a>
        <a href="logos.html" id="second"> Logos </a>
        <a href="ethos.html" id="third"> Ethos </a>
        <a href="pathos.html" id="fourth"> Pathos </a>
    </nav>
    <main>
        <section id="p1">
            The applications of AI have been increasingly tested in the past decade, and with these tests we have found
            so many ways where AI can be wrong. However, with the increasing power of AI's their mistakes have so much
            more of a devastating impact on the lives of those affected.
        </section>
        <section id="h1">
            <h2> Guilty until proven innocent </h2>
        </section>
        <section id="p2">
            <p>
                AI facial recognition is by no means a new invention, with its roots being invented in 1967 by Woodrow
                Bledsoe. Since then, this particular branch of artificial intelligence has evolved to be a whole new
                level of danger. To give a bit of context, there are two main uses of facial recognition AI:
            </p>
            <ol>
                <li><b>Identification</b>: the system takes a person's face/likeness and compares it to a large database
                    of known photos and identities to find a correct match, and it is used to identify someone when you
                    don't know who they are. </li><br>
                <li><b>Verification</b>: the system already has your likeness and is just ensuring that you're the same
                    person in the picture.</li>
            </ol>
            <p>The problems with this technology are endless, but they become especially apparent in things like police
                work and investigations. According to a report put forward by the ACLU: </p>
        </section>
        <section id="q1">
            <p>
                “...false positive rates are highest in West and East African and East Asian people, and lowest in
                Eastern European individuals. This effect is generally large, with a factor of 100 more false positives
                between countries... We found false positives to be higher in women than men, and this is consistent
                across algorithms and data sets... We found elevated false positives in the elderly and in children; the
                effects were larger in the oldest and youngest, and smallest in middle-aged adults.”<br>
                -Patrick Grother et al, 2019
            </p>
        </section>
        <section id="p3">
            <p>
                Taking this, and the other contents of the 82-page report into consideration, these kinds of facial
                recognition software are only reliably successful on middle-aged white men. The use of these software's
                should be inclusive and accurate for everyone, and the fact that they are only reliably working for this
                particular demographic of people means they have produced many different instances of false positive
                results. There are countless stories of people, who often end up being African American, who are falsely
                accused of a crime solely of the word of a faulty AI system.
            </p>
        </section>
        <section id="p4">
            <p>
                In June of 2020, Amazon, IBM, and Microsoft suspended sales of AI facial recognition software to police
                forces across the U.S. This move truly showed how big of a problem this is, as these companies are
                entirely profit based at heart and the ceasing of sales to these groups means they faced enough backlash
                for and saw enough problems with these programs that they restricted their usage. However, these
                companies are not the only creators of these AI software's, and soon enough other groups will fill this
                need, potentially resulting in more problematic and discriminatory policing practices.
            </p>
        </section>
        <section id="h2">
            <h2>A Scandal Machine</h2>
        </section>
        <section id="p5">
            <p>
                Facial recognition AIs are not the only ones making mistakes, some have taken to fabricating stories.
            Prominent figures, from a Washington legal professor to an Australian politician, have had stories about
            them be either misrepresented, or entirely fabricated, by ChatGPT. The mayor of an Australian town called
            Hepburn, Brian Hood (below), was told by ChatGPT that he was arrested for bribery and spent four years in jail.
            However, Hood was never arrested, nor did he take any bribes. In fact, he was the whistleblower on this
            entire scandal, which took place at his former job with a subsidiary of the Reserve Bank of Australia. Newer
            versions of ChatGPT have corrected this mistake, but this occurrence begs certain questions about how this
            could affect people's reputations. This same scenario was also seen in the U.S, except this AI's mistake
            came with self-created sources. Jonathan Turley is a law professor and political commentator in the U.S who
            was accused of sexually harassing a student in 2018. This revelation came after a fellow legal scholar asked
            ChatGPT for a list of legal scholars who had sexually harassed someone, and Turley's name was on that list.
            The chatbot continued, citing a Washington Post article as saying Turley made suggestive comments and
            attempted to touch a student on a class trip to Alaska. However, after Turley was emailed about it, he said
            there was no class trip to Alaska. In fact, nothing in this article had ever happened. Further investigation
            saw that this article did not exist with the Washington Post, and that it had been entirely fabricated by
            ChatGPT.
            </p>
        </section>
        <section id="img1">
            <img src="brianaustralia.jpg" alt="Brian Hood" style="float: right;" width="400px" height="400px">
        </section>
        <section id="p6">
            <p>This poses two major problems. First, that once again the widely used ChatGPT had given false, potentially
            devastating, information about a public figure. And second, that this AI now had the power to fabricate news
            articles and to use them as sources. This is the real problem with this story, as it deals a great blow to
            journalistic integrity and the ability for journalism to be taken seriously. If AI can fabricate any article
            by any organization to fit its own desires, the credibility of these organizations will be shot. This has
            been documented as happening to the Washington Post, and to the Guardian on multiple occasions. This truly
            highlights the power of AI, because not only can it tell you what the truth is, it can now back it up with
            whatever it wants. These revelations are scary to say the least, and they set a dire precedent for the
            future of journalism and AI.</p>
        </section>
    </main>
</body>

</html>